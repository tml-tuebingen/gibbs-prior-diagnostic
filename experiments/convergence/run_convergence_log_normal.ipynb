{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: XLA_FLAGS=--xla_gpu_cuda_data_dir=/usr/local/cuda/\n",
      "env: XLA_PYTHON_CLIENT_PREALLOCATE=false\n",
      "env: XLA_PYTHON_CLIENT_ALLOCATOR=platform\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%env XLA_FLAGS=--xla_gpu_cuda_data_dir=/usr/local/cuda/\n",
    "%env XLA_PYTHON_CLIENT_PREALLOCATE=false\n",
    "%env XLA_PYTHON_CLIENT_ALLOCATOR=platform\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import jax\n",
    "from jax import random\n",
    "import jax.numpy as jnp\n",
    "import numpyro as npr\n",
    "import matplotlib\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.dates as mdates\n",
    "import tqdm\n",
    "from math import *\n",
    "import numpyro.distributions as dist\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (8,5)\n",
    "matplotlib.rcParams['font.size'] = 10\n",
    "matplotlib.rcParams['font.family'] = \"serif\"\n",
    "matplotlib.rcParams['font.serif'] = 'Times'\n",
    "matplotlib.rcParams['text.usetex'] = True\n",
    "matplotlib.rcParams['lines.linewidth'] = 1\n",
    "plt = matplotlib.pyplot\n",
    "\n",
    "npr.set_platform('gpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 10\n",
    "\n",
    "def model_true(y=None, theta=None, rng_key=random.PRNGKey(1)):\n",
    "    key, *subkeys = random.split(rng_key, 5)  # subkeys\n",
    "    \n",
    "    if theta is None:\n",
    "        # Sample from priors \\pi(\\mu, \\sigma^2)\n",
    "        mu = npr.sample('mu', dist.Normal(0, 1), rng_key=subkeys[0])\n",
    "        sigma_sq = npr.sample('sigma_sq', dist.Gamma(1, 1), rng_key=subkeys[1])\n",
    "    else:\n",
    "        mu, sigma_sq = theta\n",
    "    \n",
    "    # The true likelihood, sum of LogNormal rvs.\n",
    "    with npr.plate('L', L):\n",
    "        x = npr.sample('X', dist.LogNormal(mu, jnp.sqrt(sigma_sq)), rng_key=subkeys[2])\n",
    "        \n",
    "    out = npr.sample('Y', dist.Delta(x.sum(0)), rng_key=subkeys[3], obs=y)\n",
    "        \n",
    "    if theta is None and y is not None:\n",
    "        return (mu, sigma_sq)\n",
    "    else:\n",
    "        return out\n",
    "    \n",
    "    \n",
    "def model_abc(y=None, theta=None, rng_key=random.PRNGKey(1)):\n",
    "    key, *subkeys = random.split(rng_key, 4)  # subkeys\n",
    "    \n",
    "    if theta is None:\n",
    "        # Sample from priors \\pi(\\mu, \\sigma^2)\n",
    "        mu = npr.sample('mu', dist.Normal(0, 1), rng_key=subkeys[0])\n",
    "        sigma_sq = npr.sample('sigma_sq', dist.Gamma(1, 1), rng_key=subkeys[1])\n",
    "    else:\n",
    "        mu, sigma_sq = theta\n",
    "    \n",
    "    # Approximate likelihood\n",
    "    beta_sq = jnp.log((jnp.exp(sigma_sq)-1)/L + 1)\n",
    "    alpha = mu + jnp.log(L) + 0.5*(sigma_sq - beta_sq)\n",
    "        \n",
    "    out = npr.sample('Y', dist.LogNormal(alpha, jnp.sqrt(beta_sq)), rng_key=subkeys[2], obs=y)\n",
    "    \n",
    "    if theta is None and y is not None:\n",
    "        return (mu, sigma_sq)\n",
    "    else:\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approx. posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpyro.infer import SVI, Trace_ELBO, MCMC, NUTS\n",
    "from numpyro.infer.autoguide import *\n",
    "\n",
    "\n",
    "def laplace(rng_key, model, y, pbar=False):    \n",
    "    key, *subkeys = random.split(rng_key, 4)\n",
    "    \n",
    "    guide = AutoLaplaceApproximation(model)\n",
    "    lr = 1e-3\n",
    "    n_iter = 5000\n",
    "\n",
    "    optimizer = npr.optim.ClippedAdam(step_size=lr)\n",
    "    svi = SVI(model, guide, optimizer, loss=Trace_ELBO(num_particles=1))\n",
    "    svi_result = svi.run(rng_key, n_iter, y=y, progress_bar=pbar)\n",
    "    \n",
    "    mu = guide.sample_posterior(subkeys[1], svi_result.params)['mu']\n",
    "    sigma_sq = guide.sample_posterior(subkeys[2], svi_result.params)['sigma_sq']\n",
    "    \n",
    "    return mu, sigma_sq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling $\\pi_G$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_gibbs_prior(rng_key, n_chains=5, T=1000):\n",
    "    r_hats, autocorrs = [], []\n",
    "\n",
    "    theta_samples = [[] for i in range(n_chains)]\n",
    "    y_ts = []\n",
    "    \n",
    "    for i in range(n_chains):\n",
    "        rng_key, subkey = random.split(rng_key)\n",
    "        y_t_i = model_true(y=None, theta=None, rng_key=subkey)\n",
    "        y_ts.append(100*y_t_i)  # Overdispersed initialization\n",
    "    \n",
    "    for t in tqdm.trange(T): \n",
    "        for i in range(n_chains):\n",
    "            rng_key, *subkeys = random.split(rng_key, 3)\n",
    "            \n",
    "            # Get q(theta | y_t)\n",
    "            theta_t_i = laplace(subkeys[0], model_abc, y=y_ts[i])              \n",
    "            theta_samples[i].append(np.array(theta_t_i).copy())\n",
    "\n",
    "            # Sample y_t\n",
    "            y_ts[i]  = model_true(y=None, theta=theta_t_i, rng_key=subkeys[1])\n",
    "            \n",
    "            while y_ts[i] == inf:\n",
    "                rng_key, subkey = random.split(rng_key)\n",
    "                y_ts[i]  = model_true(y=None, theta=theta_t_i, rng_key=subkey)\n",
    "        \n",
    "    # Shape: (n_chains, n_samples, n_dim)\n",
    "    return np.array(theta_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1000, 2)\n"
     ]
    }
   ],
   "source": [
    "theta_samples = sample_gibbs_prior(random.PRNGKey(1234), n_chains=5, T=500)\n",
    "theta_samples = np.array(theta_samples)\n",
    "\n",
    "np.save('../../results/convergence/multi_chains_laplace.npy', theta_samples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}